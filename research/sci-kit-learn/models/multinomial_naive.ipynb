{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats of training set:  (159571, 7)\n",
      "Labels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Below option allows us to see the entire comment_text column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Read in the dataset\n",
    "train = pd.read_csv(\"../../data/kaggle_train.csv\")\n",
    "train = train.drop(columns=['id'])\n",
    "\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "print(\"Stats of training set: \", train.shape)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nMore\\r\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Below I have noticed some inconsistencies in the data and by preprocessing it, we can ensure a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert comment to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(to_lowercase)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Remove HTML tags from the comments\n",
    "def remove_html(text):\n",
    "    return re.sub(r\"<.*>\", \"\", text, flags=re.MULTILINE)\n",
    "    \n",
    "train['comment_text'] = train['comment_text'].apply(remove_html)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove links from the comments\n",
    "def remove_links(text):\n",
    "    text= re.sub(r\"http\\S+\",\" \",text, flags=re.MULTILINE)\n",
    "    return re.sub(r\"www\\S+\",\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_links)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\r\\nmore\\r\\ni cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember what page thats on</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                      explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                    hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info   \n",
       "3  \\r\\nmore\\r\\ni cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport     \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you sir are my hero any chance you remember what page thats on   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "# Remove punctuation marks \n",
    "def remove_punctuation(text):\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i, \"\")\n",
    "    return text\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_punctuation)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation  why the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more  i cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know    there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember what page thats on</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                explanation  why the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                            hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info   \n",
       "3    more  i cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know    there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport     \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 you sir are my hero any chance you remember what page thats on   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove special characters such as: \\n \\r \\t\n",
    "def remove_special(text):\n",
    "    return re.sub(r\"[\\n\\t\\\\\\/\\r]\",\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_special)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww matches background colour im seemingly stuck thanks talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                comment_text  \\\n",
       "0                                                                                                                                                                                                 explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                     daww matches background colour im seemingly stuck thanks talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                           hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info   \n",
       "3  cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport   \n",
       "4                                                                                                                                                                                                                                                                                                                                                        sir hero chance remember page thats   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords using nltk's stopwords package\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww matches background colour im seemingly stuck thanks talk      january         utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                comment_text  \\\n",
       "0                                                                                                                                                                                                 explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now            \n",
       "1                                                                                                                                                                                                                                                                                                     daww matches background colour im seemingly stuck thanks talk      january         utc   \n",
       "2                                                                                                                                                                                                                                           hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info   \n",
       "3  cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport   \n",
       "4                                                                                                                                                                                                                                                                                                                                                        sir hero chance remember page thats   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see above, there are numbers and/or dates\n",
    "# I will remove those as they are not helpful\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d',\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_numbers)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test, and Validation Split\n",
    "\n",
    "Below I need to split the dataset into train and test datasets.\n",
    "However, sklearn's `train_test_split` function does not work for\n",
    "multi-class classification.\n",
    "\n",
    "Therefore, I will be creating a train, test, and validation split for each label in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36870</th>\n",
       "      <td>jersey shore steel created jersey shore steel tonight read much like commercial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149148</th>\n",
       "      <td>redirect user talktony tan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36275</th>\n",
       "      <td>ygm title implies new email please respond email im going disable javascript second time master annatar great bids thee welcome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104123</th>\n",
       "      <td>synth removed correctly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21814</th>\n",
       "      <td>carbay hi thanks message added much know company researched apps present market dont news quote cited google play store hope fine page reviewed help reviewing first page curious thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27015</th>\n",
       "      <td>worth constitution also uses upper case whats doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23711</th>\n",
       "      <td>also cant seem end redirecti wanted create article names saad imtiaz instead redirects user pagecan tell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158476</th>\n",
       "      <td>get soo offended call airhead isnt even offensive love always make laugh try befriend lol real life obv god knows find offensive blocked due jeez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38826</th>\n",
       "      <td>youre tyrant youre deleting edits angeliques surname carrington rango movie go angeliques facebook page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>agree dont really see anyone could really confuse two point coming w say leave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127656 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                    comment_text\n",
       "36870                                                                                                            jersey shore steel created jersey shore steel tonight read much like commercial\n",
       "149148                                                                                                                                                             redirect user talktony tan   \n",
       "36275                                                            ygm title implies new email please respond email im going disable javascript second time master annatar great bids thee welcome\n",
       "104123                                                                                                                                                                   synth removed correctly\n",
       "21814   carbay hi thanks message added much know company researched apps present market dont news quote cited google play store hope fine page reviewed help reviewing first page curious thanks\n",
       "...                                                                                                                                                                                          ...\n",
       "27015                                                                                                                                          worth constitution also uses upper case whats doc\n",
       "23711                                                                                   also cant seem end redirecti wanted create article names saad imtiaz instead redirects user pagecan tell\n",
       "158476                                         get soo offended call airhead isnt even offensive love always make laugh try befriend lol real life obv god knows find offensive blocked due jeez\n",
       "38826                                                                        youre tyrant youre deleting edits angeliques surname carrington rango movie go angeliques facebook page            \n",
       "6916                                                                                                              agree dont really see anyone could really confuse two point coming w say leave\n",
       "\n",
       "[127656 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[[\"comment_text\"]], train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]], test_size=0.20)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (127656, 1)\n",
      "Test shape: (31915, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\",X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing the Comment Text\n",
    "\n",
    "*Logistic Regression can't take text values as input*\n",
    "\n",
    "Since the independent variable I have is only text, we will need to use a vectorizer to convert the text into usable data for Logistic Regression.\n",
    "\n",
    "```\n",
    "\n",
    "# Max_features = Build a vocabulary that only consider the top max_features ordered by term frequency\n",
    "\n",
    "# Analyzer = Whether the feature should be made of word or character n-grams. Option char_wb creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "# ngram_range = (1,1) means only unigrams, (1,2) means unigrams and bigrams, (1,3) means unigrams, bigrams, and trigrams\n",
    "\n",
    "# Further ngrams knowledge = bigrams means it will learn the occurence of every two words, trigrams would be every 3, etc.\n",
    "\n",
    "# dtype = type of the matrix returned, default is float64\n",
    "```\n",
    "\n",
    "We will use a word and char n-grams as some people like to obfuscate words by using multiple characters, by using both we can hope to catch these.\n",
    "The idea from this came from [here](https://www.kaggle.com/code/tunguz/logistic-regression-with-words-and-char-n-grams/comments) which has one of the best results for Logistic Regression. This user optimized the ngram_range.\n",
    "\n",
    "We use FeatureUnion (similar to how hstack works in previous non-Pipeline example) to combine the word and char n-ngrams as described in this [post](https://stackoverflow.com/questions/65765954/word-and-char-ngram-with-different-ngram-range-on-tfidfvectorizer-pipeline) into one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "cols_trans = ColumnTransformer([\n",
    "    (\"txt_word\", TfidfVectorizer(max_features=10000, analyzer=\"word\", ngram_range=(1,3), dtype=np.float32), 'comment_text'),\n",
    "    (\"txt_char\", TfidfVectorizer(max_features=10000, analyzer=\"char\", ngram_range=(3,6), dtype=np.float32), 'comment_text')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "\n",
    "Create a Pipeline for the data to flow through:\n",
    "\n",
    "TFIDF Vectorize the data\n",
    "\n",
    "then\n",
    "\n",
    "Perform Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('trans', cols_trans),\n",
    "    ('clf', MultinomialNB())\n",
    "], memory=\"tmp/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(memory=&#x27;tmp/cache&#x27;,\n",
       "         steps=[(&#x27;trans&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                                  TfidfVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  &#x27;comment_text&#x27;),\n",
       "                                                 (&#x27;txt_char&#x27;,\n",
       "                                                  TfidfVectorizer(analyzer=&#x27;char&#x27;,\n",
       "                                                                  dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  &#x27;comment_text&#x27;)])),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(memory=&#x27;tmp/cache&#x27;,\n",
       "         steps=[(&#x27;trans&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                                  TfidfVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  &#x27;comment_text&#x27;),\n",
       "                                                 (&#x27;txt_char&#x27;,\n",
       "                                                  TfidfVectorizer(analyzer=&#x27;char&#x27;,\n",
       "                                                                  dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  &#x27;comment_text&#x27;)])),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">trans: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                 TfidfVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                 max_features=10000,\n",
       "                                                 ngram_range=(1, 3)),\n",
       "                                 &#x27;comment_text&#x27;),\n",
       "                                (&#x27;txt_char&#x27;,\n",
       "                                 TfidfVectorizer(analyzer=&#x27;char&#x27;,\n",
       "                                                 dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                 max_features=10000,\n",
       "                                                 ngram_range=(3, 6)),\n",
       "                                 &#x27;comment_text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">txt_word</label><div class=\"sk-toggleable__content\"><pre>comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, max_features=10000,\n",
       "                ngram_range=(1, 3))</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">txt_char</label><div class=\"sk-toggleable__content\"><pre>comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                max_features=10000, ngram_range=(3, 6))</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(memory='tmp/cache',\n",
       "         steps=[('trans',\n",
       "                 ColumnTransformer(transformers=[('txt_word',\n",
       "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  'comment_text'),\n",
       "                                                 ('txt_char',\n",
       "                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                  dtype=<class 'numpy.float32'>,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  'comment_text')])),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "# with display='diagram', simply use display() to see the diagram\n",
    "display(pipe)\n",
    "# if desired, set display back to the default\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Below I will build and train the `Logistic Regression` model and check if the model is overfit, underfit, or optimal fit using GridSearch I will find the best hyperparameters.\n",
    "\n",
    "We create a new model for each label in order to classify the multi-class, example we cross validate the `toxic` label, then the `severe_toxic` and so on. This method is the preferred method based on previous implementations for the Kaggle competition.\n",
    "\n",
    "By  doing this, we can evaluate the percentage for each label and choose the highest label(s) which we should classify the text as. For example, in the data we have data which may be `toxic` and `obscene` rather than only `toxic` data and only `obscene` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Values: \n",
      "0.0\n",
      "0.001\n",
      "0.004641588833612777\n",
      "0.021544346900318832\n",
      "0.1\n",
      "0.46415888336127775\n",
      "2.154434690031882\n",
      "10.0\n",
      "46.41588833612773\n",
      "215.44346900318823\n",
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "# speecify parameter values to search\n",
    "alpha = np.logspace(-3, 3, 10)\n",
    "alpha = np.append(0, alpha)\n",
    "\n",
    "print(\"Alpha Values: \")\n",
    "for c in alpha:\n",
    "    print(c)\n",
    "params = {}\n",
    "params['clf__alpha'] = alpha\n",
    "params['clf__fit_prior'] = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.98s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.07s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.53s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 24.32s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tune [toxic]: 2666.3353753089905\n",
      "\tBest Score: 0.9445932897247424\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=2.154434690031882))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.52s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.92s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.49s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.49s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.40s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 25.94s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tune [severe_toxic]: 2760.6875436306\n",
      "\tBest Score: 0.9902472269199599\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=215.44346900318823))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.04s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.93s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.12s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.47s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.63s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 25.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tune [obscene]: 2750.9867374897003\n",
      "\tBest Score: 0.9717443738454417\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=2.154434690031882))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.86s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.30s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.05s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.47s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.05s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 26.19s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tune [threat]: 2902.0564937591553\n",
      "\tBest Score: 0.9970075830623422\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=46.41588833612773))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.66s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.83s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.47s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.42s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.62s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.93s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tune [insult]: 2740.856691122055\n",
      "\tBest Score: 0.9655558785845788\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=2.154434690031882))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.87s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.64s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.12s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.84s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 24.46s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tune [identity_hate]: 2752.17586684227\n",
      "\tBest Score: 0.9912029207758188\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=46.41588833612773))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore') \n",
    "\n",
    "best_results = {}\n",
    "\n",
    "for label in labels:\n",
    "    start = time.time()\n",
    "\n",
    "    grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train[label])\n",
    "\n",
    "    best_results[label] = {\n",
    "        \"score\": grid.best_score_,\n",
    "        \"parameters\": grid.best_params_,\n",
    "        \"estimator\": grid.best_estimator_\n",
    "    }\n",
    "\n",
    "    print(f\"Time to tune [{label}]: {time.time() - start}\")\n",
    "    print(f\"\\tBest Score: {grid.best_score_}\")\n",
    "    print(f\"\\tFinal Model:: {grid.best_estimator_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.944593</td>\n",
       "      <td>0.990247</td>\n",
       "      <td>0.971744</td>\n",
       "      <td>0.997008</td>\n",
       "      <td>0.965556</td>\n",
       "      <td>0.991203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parameters</th>\n",
       "      <td>{'clf__alpha': 2.154434690031882, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 215.44346900318823, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 2.154434690031882, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 46.41588833612773, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 2.154434690031882, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 46.41588833612773, 'clf__fit_prior': True}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimator</th>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=2.154434690031882))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=215.44346900318823))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=2.154434690031882))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=46.41588833612773))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=2.154434690031882))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=46.41588833612773))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 toxic  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.944593   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'clf__alpha': 2.154434690031882, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=2.154434690031882))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           severe_toxic  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.990247   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'clf__alpha': 215.44346900318823, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=215.44346900318823))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               obscene  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.971744   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'clf__alpha': 2.154434690031882, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=2.154434690031882))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                threat  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.997008   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'clf__alpha': 46.41588833612773, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=46.41588833612773))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                insult  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.965556   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'clf__alpha': 2.154434690031882, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=2.154434690031882))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         identity_hate  \n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.991203  \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   {'clf__alpha': 46.41588833612773, 'clf__fit_prior': True}  \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), MultinomialNB(alpha=46.41588833612773))  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = pd.DataFrame.from_dict(best_results)\n",
    "final_results.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9767252121521474"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.iloc[0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=2.154434690031882))])\n",
      "severe_toxic Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=215.44346900318823))])\n",
      "obscene Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=2.154434690031882))])\n",
      "threat Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=46.41588833612773))])\n",
      "insult Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=2.154434690031882))])\n",
      "identity_hate Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', MultinomialNB(alpha=46.41588833612773))])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "for k, v in best_results.items():\n",
    "    print(k, v['estimator'])\n",
    "    joblib.dump(v['estimator'], f'F:/Thesis/models/multinomial_naive/{k}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "models = {}\n",
    "\n",
    "for label in labels:\n",
    "    models[label] = joblib.load(open(f'F:/Thesis/models/logistic/{label}.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.65s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.86s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.81s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.41s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.85s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------toxic-----------\n",
      "[[114100   1297]\n",
      " [  3768   8491]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    115397\n",
      "           1       0.87      0.69      0.77     12259\n",
      "\n",
      "    accuracy                           0.96    127656\n",
      "   macro avg       0.92      0.84      0.87    127656\n",
      "weighted avg       0.96      0.96      0.96    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 24.85s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.15s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.32s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.93s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------severe_toxic-----------\n",
      "[[126047    337]\n",
      " [   909    363]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    126384\n",
      "           1       0.52      0.29      0.37      1272\n",
      "\n",
      "    accuracy                           0.99    127656\n",
      "   macro avg       0.76      0.64      0.68    127656\n",
      "weighted avg       0.99      0.99      0.99    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.31s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.31s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.77s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.40s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.65s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------obscene-----------\n",
      "[[120192    685]\n",
      " [  1795   4984]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    120877\n",
      "           1       0.88      0.74      0.80      6779\n",
      "\n",
      "    accuracy                           0.98    127656\n",
      "   macro avg       0.93      0.86      0.90    127656\n",
      "weighted avg       0.98      0.98      0.98    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.82s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 24.28s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.54s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------threat-----------\n",
      "[[127222     59]\n",
      " [   275    100]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    127281\n",
      "           1       0.63      0.27      0.37       375\n",
      "\n",
      "    accuracy                           1.00    127656\n",
      "   macro avg       0.81      0.63      0.69    127656\n",
      "weighted avg       1.00      1.00      1.00    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.41s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 24.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.78s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.41s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------insult-----------\n",
      "[[120382    932]\n",
      " [  2650   3692]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99    121314\n",
      "           1       0.80      0.58      0.67      6342\n",
      "\n",
      "    accuracy                           0.97    127656\n",
      "   macro avg       0.89      0.79      0.83    127656\n",
      "weighted avg       0.97      0.97      0.97    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.96s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.93s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.36s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------identity_hate-----------\n",
      "[[126345    174]\n",
      " [   817    320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    126519\n",
      "           1       0.65      0.28      0.39      1137\n",
      "\n",
      "    accuracy                           0.99    127656\n",
      "   macro avg       0.82      0.64      0.69    127656\n",
      "weighted avg       0.99      0.99      0.99    127656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "for k, v in models.items():\n",
    "    y_actual = y_train[k]\n",
    "    y_predicted = cross_val_predict(v, X_train, y_train[k], cv=5)\n",
    "    print(f\"-----------{k}-----------\")\n",
    "    print(metrics.confusion_matrix(y_actual, y_predicted))\n",
    "    print(metrics.classification_report(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
