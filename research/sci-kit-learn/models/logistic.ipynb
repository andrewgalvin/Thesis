{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats of training set:  (159571, 7)\n",
      "Labels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Below option allows us to see the entire comment_text column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Read in the dataset\n",
    "train = pd.read_csv(\"../../data/kaggle_train.csv\")\n",
    "train = train.drop(columns=['id'])\n",
    "\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "print(\"Stats of training set: \", train.shape)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nMore\\r\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Below I have noticed some inconsistencies in the data and by preprocessing it, we can ensure a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert comment to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(to_lowercase)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Remove HTML tags from the comments\n",
    "def remove_html(text):\n",
    "    return re.sub(r\"<.*>\", \"\", text, flags=re.MULTILINE)\n",
    "    \n",
    "train['comment_text'] = train['comment_text'].apply(remove_html)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove links from the comments\n",
    "def remove_links(text):\n",
    "    text= re.sub(r\"http\\S+\",\" \",text, flags=re.MULTILINE)\n",
    "    return re.sub(r\"www\\S+\",\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_links)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\r\\nmore\\r\\ni cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember what page thats on</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                      explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                    hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info   \n",
       "3  \\r\\nmore\\r\\ni cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport     \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you sir are my hero any chance you remember what page thats on   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "# Remove punctuation marks \n",
    "def remove_punctuation(text):\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i, \"\")\n",
    "    return text\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_punctuation)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation  why the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more  i cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know    there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember what page thats on</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                explanation  why the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                            hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info   \n",
       "3    more  i cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know    there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport     \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 you sir are my hero any chance you remember what page thats on   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove special characters such as: \\n \\r \\t\n",
    "def remove_special(text):\n",
    "    return re.sub(r\"[\\n\\t\\\\\\/\\r]\",\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_special)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww matches background colour im seemingly stuck thanks talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                comment_text  \\\n",
       "0                                                                                                                                                                                                 explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                     daww matches background colour im seemingly stuck thanks talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                           hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info   \n",
       "3  cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport   \n",
       "4                                                                                                                                                                                                                                                                                                                                                        sir hero chance remember page thats   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords using nltk's stopwords package\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww matches background colour im seemingly stuck thanks talk      january         utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                comment_text  \\\n",
       "0                                                                                                                                                                                                 explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now            \n",
       "1                                                                                                                                                                                                                                                                                                     daww matches background colour im seemingly stuck thanks talk      january         utc   \n",
       "2                                                                                                                                                                                                                                           hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info   \n",
       "3  cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport   \n",
       "4                                                                                                                                                                                                                                                                                                                                                        sir hero chance remember page thats   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see above, there are numbers and/or dates\n",
    "# I will remove those as they are not helpful\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d',\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_numbers)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test, and Validation Split\n",
    "\n",
    "Below I need to split the dataset into train and test datasets.\n",
    "However, sklearn's `train_test_split` function does not work for\n",
    "multi-class classification.\n",
    "\n",
    "Therefore, I will be creating a train, test, and validation split for each label in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77333</th>\n",
       "      <td>wikipedia ambassador update hi youre getting message wikipedia ambassador new term beginning united states canada education programs wanted give update important new information youre interested continuing work term wikipedia ambassador may heard reference transition education program going last term wikimedia foundation directly run us canada programs beginning june proposed thematic organization likely take organizing program read proposal another major change program take effect immediately beginning term new mediawiki education extension replace course pages ambassador lists see wikipediacourse pages helpeducation program extension details included extension online volunteer campus volunteer user rights let create edit course pages sign ambassador particular course would like continue serving wikipedia ambassador  even support class term  must create ambassador profile youre longer interested wikipedia ambassador dont need anything please steps soon possible first need relevant user rights online andor campus ambassadors admin grant rights well ambassadors post rights request well get set quickly possible youve got ambassador rights please set campus andor online ambassador profile specialcampusambassadorprofile specialonlineambassadorprofile going forward lists ambassadors specialcampusambassadors specialonlineambassadors official roster active ambassador would like ambassador ready serve term uncheck option profile publicly list remove profile list sign support courses list courses specialcourses default lists current courses change status filter planned see courses term havent reached listed start date yet first term used extension know bugs know feature set rich could big wave improvements already pipeline know mediawiki could help code review wed love help please reach sage ross complaints bug reports feature suggestions basic features extension documented wikipediacourse pages see tutorial setting using communication keeping date past education program pretty fragmented set communication channels trying fix recommended places discuss stay uptodate education program education noticeboard become main onwiki location discussion education program post broad education program issues well issues individual courses ambassadors announce email list lowtraffic announcements list important information ambassadors need aware encourage ambassadors interested wikipedians subscribe list follow instructions link add email address use irc regularly need try reach someone immediately irc channel place find fellow ambassadors ambassador training resources online training ambassadors intended orientation wikipedia ambassador role newcomers manual role parallel trainings students educators well please go training feel like need refresher typical class supposed go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56517</th>\n",
       "      <td>utc  singapore  acor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151047</th>\n",
       "      <td>also support point view blocked article point virtually contributions pointless edits waste time however fully agree idea article sanctionned behaviour users edit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39485</th>\n",
       "      <td>notability hes really notable enough warrant standalone article merge redirect resident evil dead aim someone convince otherwise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97994</th>\n",
       "      <td>also supports white nationalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114094</th>\n",
       "      <td>agree move</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135729</th>\n",
       "      <td>anything like saying hypothesis global warming doesnt significant scientific support im saying doesnt wrong suggest theres scientific support anthropogenic gw saying scientific support global cooling perhaps reason much support gw global cooling   s information studies much available days maybe reason gw big deal places like myspace mtv make much marketable personally see lot hype lot proof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136183</th>\n",
       "      <td>maldivian language official common never referred language regulaters languages world seen vandalisations languages shouldnt mess youre good mind business go get educated come back places like contribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157755</th>\n",
       "      <td> whatever trying say tbnl seems disconcerting threatening also rather incoherent would mind restating want say clarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135467</th>\n",
       "      <td>warning wasnt racist sexist harming anyone anyway team would like reason person deleated within next    hours</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127656 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                comment_text\n",
       "77333   wikipedia ambassador update hi youre getting message wikipedia ambassador new term beginning united states canada education programs wanted give update important new information youre interested continuing work term wikipedia ambassador may heard reference transition education program going last term wikimedia foundation directly run us canada programs beginning june proposed thematic organization likely take organizing program read proposal another major change program take effect immediately beginning term new mediawiki education extension replace course pages ambassador lists see wikipediacourse pages helpeducation program extension details included extension online volunteer campus volunteer user rights let create edit course pages sign ambassador particular course would like continue serving wikipedia ambassador  even support class term  must create ambassador profile youre longer interested wikipedia ambassador dont need anything please steps soon possible first need relevant user rights online andor campus ambassadors admin grant rights well ambassadors post rights request well get set quickly possible youve got ambassador rights please set campus andor online ambassador profile specialcampusambassadorprofile specialonlineambassadorprofile going forward lists ambassadors specialcampusambassadors specialonlineambassadors official roster active ambassador would like ambassador ready serve term uncheck option profile publicly list remove profile list sign support courses list courses specialcourses default lists current courses change status filter planned see courses term havent reached listed start date yet first term used extension know bugs know feature set rich could big wave improvements already pipeline know mediawiki could help code review wed love help please reach sage ross complaints bug reports feature suggestions basic features extension documented wikipediacourse pages see tutorial setting using communication keeping date past education program pretty fragmented set communication channels trying fix recommended places discuss stay uptodate education program education noticeboard become main onwiki location discussion education program post broad education program issues well issues individual courses ambassadors announce email list lowtraffic announcements list important information ambassadors need aware encourage ambassadors interested wikipedians subscribe list follow instructions link add email address use irc regularly need try reach someone immediately irc channel place find fellow ambassadors ambassador training resources online training ambassadors intended orientation wikipedia ambassador role newcomers manual role parallel trainings students educators well please go training feel like need refresher typical class supposed go\n",
       "56517                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  utc  singapore  acor\n",
       "151047                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    also support point view blocked article point virtually contributions pointless edits waste time however fully agree idea article sanctionned behaviour users edit\n",
       "39485                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       notability hes really notable enough warrant standalone article merge redirect resident evil dead aim someone convince otherwise\n",
       "97994                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        also supports white nationalism\n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ...\n",
       "114094                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            agree move\n",
       "135729                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             anything like saying hypothesis global warming doesnt significant scientific support im saying doesnt wrong suggest theres scientific support anthropogenic gw saying scientific support global cooling perhaps reason much support gw global cooling   s information studies much available days maybe reason gw big deal places like myspace mtv make much marketable personally see lot hype lot proof\n",
       "136183                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           maldivian language official common never referred language regulaters languages world seen vandalisations languages shouldnt mess youre good mind business go get educated come back places like contribute\n",
       "157755                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                whatever trying say tbnl seems disconcerting threatening also rather incoherent would mind restating want say clarity\n",
       "135467                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         warning wasnt racist sexist harming anyone anyway team would like reason person deleated within next    hours\n",
       "\n",
       "[127656 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[[\"comment_text\"]], train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]], test_size=0.20)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (127656, 1)\n",
      "Test shape: (31915, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\",X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing the Comment Text\n",
    "\n",
    "*Logistic Regression can't take text values as input*\n",
    "\n",
    "Since the independent variable I have is only text, we will need to use a vectorizer to convert the text into usable data for Logistic Regression.\n",
    "\n",
    "```\n",
    "\n",
    "# Max_features = Build a vocabulary that only consider the top max_features ordered by term frequency\n",
    "\n",
    "# Analyzer = Whether the feature should be made of word or character n-grams. Option char_wb creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "# ngram_range = (1,1) means only unigrams, (1,2) means unigrams and bigrams, (1,3) means unigrams, bigrams, and trigrams\n",
    "\n",
    "# Further ngrams knowledge = bigrams means it will learn the occurence of every two words, trigrams would be every 3, etc.\n",
    "\n",
    "# dtype = type of the matrix returned, default is float64\n",
    "```\n",
    "\n",
    "We will use a word and char n-grams as some people like to obfuscate words by using multiple characters, by using both we can hope to catch these.\n",
    "The idea from this came from [here](https://www.kaggle.com/code/tunguz/logistic-regression-with-words-and-char-n-grams/comments) which has one of the best results for Logistic Regression. This user optimized the ngram_range.\n",
    "\n",
    "We use FeatureUnion (similar to how hstack works in previous non-Pipeline example) to combine the word and char n-ngrams as described in this [post](https://stackoverflow.com/questions/65765954/word-and-char-ngram-with-different-ngram-range-on-tfidfvectorizer-pipeline) into one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "cols_trans = ColumnTransformer([\n",
    "    (\"txt_word\", TfidfVectorizer(max_features=10000, analyzer=\"word\", ngram_range=(1,3), dtype=np.float32), 'comment_text'),\n",
    "    (\"txt_char\", TfidfVectorizer(max_features=10000, analyzer=\"char\", ngram_range=(3,6), dtype=np.float32), 'comment_text')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "\n",
    "Create a Pipeline for the data to flow through:\n",
    "\n",
    "TFIDF Vectorize the data\n",
    "\n",
    "then\n",
    "\n",
    "Perform Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('trans', cols_trans),\n",
    "    ('clf', LogisticRegression())\n",
    "], memory=\"tmp/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(memory=&#x27;tmp/cache&#x27;,\n",
       "         steps=[(&#x27;trans&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                                  TfidfVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  &#x27;comment_text&#x27;),\n",
       "                                                 (&#x27;txt_char&#x27;,\n",
       "                                                  TfidfVectorizer(analyzer=&#x27;char&#x27;,\n",
       "                                                                  dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  &#x27;comment_text&#x27;)])),\n",
       "                (&#x27;clf&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(memory=&#x27;tmp/cache&#x27;,\n",
       "         steps=[(&#x27;trans&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                                  TfidfVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  &#x27;comment_text&#x27;),\n",
       "                                                 (&#x27;txt_char&#x27;,\n",
       "                                                  TfidfVectorizer(analyzer=&#x27;char&#x27;,\n",
       "                                                                  dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  &#x27;comment_text&#x27;)])),\n",
       "                (&#x27;clf&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">trans: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                 TfidfVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                 max_features=10000,\n",
       "                                                 ngram_range=(1, 3)),\n",
       "                                 &#x27;comment_text&#x27;),\n",
       "                                (&#x27;txt_char&#x27;,\n",
       "                                 TfidfVectorizer(analyzer=&#x27;char&#x27;,\n",
       "                                                 dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                 max_features=10000,\n",
       "                                                 ngram_range=(3, 6)),\n",
       "                                 &#x27;comment_text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">txt_word</label><div class=\"sk-toggleable__content\"><pre>comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, max_features=10000,\n",
       "                ngram_range=(1, 3))</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">txt_char</label><div class=\"sk-toggleable__content\"><pre>comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                max_features=10000, ngram_range=(3, 6))</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(memory='tmp/cache',\n",
       "         steps=[('trans',\n",
       "                 ColumnTransformer(transformers=[('txt_word',\n",
       "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  'comment_text'),\n",
       "                                                 ('txt_char',\n",
       "                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                  dtype=<class 'numpy.float32'>,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  'comment_text')])),\n",
       "                ('clf', LogisticRegression())])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "# with display='diagram', simply use display() to see the diagram\n",
    "display(pipe)\n",
    "# if desired, set display back to the default\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Below I will build and train the `Logistic Regression` model and check if the model is overfit, underfit, or optimal fit using GridSearch I will find the best hyperparameters.\n",
    "\n",
    "We create a new model for each label in order to classify the multi-class, example we cross validate the `toxic` label, then the `severe_toxic` and so on. This method is the preferred method based on previous implementations for the Kaggle competition.\n",
    "\n",
    "By  doing this, we can evaluate the percentage for each label and choose the highest label(s) which we should classify the text as. For example, in the data we have data which may be `toxic` and `obscene` rather than only `toxic` data and only `obscene` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Values: \n",
      "0.001\n",
      "0.004641588833612777\n",
      "0.021544346900318832\n",
      "0.1\n",
      "0.46415888336127775\n",
      "2.154434690031882\n",
      "10.0\n",
      "46.41588833612773\n",
      "215.44346900318823\n",
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "# speecify parameter values to search\n",
    "C = np.logspace(-3, 3, 10)\n",
    "print(\"C Values: \")\n",
    "for c in C:\n",
    "    print(c)\n",
    "params = {}\n",
    "params['clf__C'] = C\n",
    "params['clf__penalty'] = ['l1','l2']\n",
    "params['clf__solver'] = ['sag', 'saga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tune [toxic]: 13540.52413892746\n",
      "\tBest Score: 0.9602133798900943\n",
      "\tBest C value: 2.154434690031882\n",
      "\tBest Penalty value: l1\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=2.154434690031882, penalty='l1',\n",
      "                                    solver='saga'))])\n",
      "Time to tune [severe_toxic]: 11487.723016738892\n",
      "\tBest Score: 0.9907564066165554\n",
      "\tBest C value: 2.154434690031882\n",
      "\tBest Penalty value: l2\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', LogisticRegression(C=2.154434690031882, solver='sag'))])\n",
      "Time to tune [obscene]: 12976.735292673111\n",
      "\tBest Score: 0.9809879650811777\n",
      "\tBest C value: 2.154434690031882\n",
      "\tBest Penalty value: l1\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=2.154434690031882, penalty='l1',\n",
      "                                    solver='saga'))])\n",
      "Time to tune [threat]: 10157.380200624466\n",
      "\tBest Score: 0.9973757607744117\n",
      "\tBest C value: 10.0\n",
      "\tBest Penalty value: l2\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', LogisticRegression(C=10.0, solver='sag'))])\n",
      "Time to tune [insult]: 13545.480309963226\n",
      "\tBest Score: 0.9721360560814111\n",
      "\tBest C value: 2.154434690031882\n",
      "\tBest Penalty value: l2\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=2.154434690031882, solver='saga'))])\n",
      "Time to tune [identity_hate]: 12064.297194242477\n",
      "\tBest Score: 0.9926129590300082\n",
      "\tBest C value: 2.154434690031882\n",
      "\tBest Penalty value: l2\n",
      "\tFinal Model:: Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', LogisticRegression(C=2.154434690031882, solver='sag'))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import warnings\n",
    "# warnings.filterwarnings('ignore') \n",
    "\n",
    "best_results = {}\n",
    "\n",
    "for label in labels:\n",
    "    start = time.time()\n",
    "\n",
    "    grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train[label])\n",
    "\n",
    "    best_results[label] = {\n",
    "        \"score\": grid.best_score_,\n",
    "        \"parameters\": grid.best_params_,\n",
    "        \"estimator\": grid.best_estimator_\n",
    "    }\n",
    "\n",
    "    print(f\"Time to tune [{label}]: {time.time() - start}\")\n",
    "    print(f\"\\tBest Score: {grid.best_score_}\")\n",
    "    print(f\"\\tBest C value: {grid.best_params_['clf__C']}\")\n",
    "    print(f\"\\tBest Penalty value: {grid.best_params_['clf__penalty']}\")\n",
    "    print(f\"\\tFinal Model:: {grid.best_estimator_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': {'score': 0.9602133798900943,\n",
       "  'parameters': {'clf__C': 2.154434690031882,\n",
       "   'clf__penalty': 'l1',\n",
       "   'clf__solver': 'saga'},\n",
       "  'estimator': Pipeline(memory='tmp/cache',\n",
       "           steps=[('trans',\n",
       "                   ColumnTransformer(transformers=[('txt_word',\n",
       "                                                    TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 3)),\n",
       "                                                    'comment_text'),\n",
       "                                                   ('txt_char',\n",
       "                                                    TfidfVectorizer(analyzer='char',\n",
       "                                                                    dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(3,\n",
       "                                                                                 6)),\n",
       "                                                    'comment_text')])),\n",
       "                  ('clf',\n",
       "                   LogisticRegression(C=2.154434690031882, penalty='l1',\n",
       "                                      solver='saga'))])},\n",
       " 'severe_toxic': {'score': 0.9907564066165554,\n",
       "  'parameters': {'clf__C': 2.154434690031882,\n",
       "   'clf__penalty': 'l2',\n",
       "   'clf__solver': 'sag'},\n",
       "  'estimator': Pipeline(memory='tmp/cache',\n",
       "           steps=[('trans',\n",
       "                   ColumnTransformer(transformers=[('txt_word',\n",
       "                                                    TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 3)),\n",
       "                                                    'comment_text'),\n",
       "                                                   ('txt_char',\n",
       "                                                    TfidfVectorizer(analyzer='char',\n",
       "                                                                    dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(3,\n",
       "                                                                                 6)),\n",
       "                                                    'comment_text')])),\n",
       "                  ('clf', LogisticRegression(C=2.154434690031882, solver='sag'))])},\n",
       " 'obscene': {'score': 0.9809879650811777,\n",
       "  'parameters': {'clf__C': 2.154434690031882,\n",
       "   'clf__penalty': 'l1',\n",
       "   'clf__solver': 'saga'},\n",
       "  'estimator': Pipeline(memory='tmp/cache',\n",
       "           steps=[('trans',\n",
       "                   ColumnTransformer(transformers=[('txt_word',\n",
       "                                                    TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 3)),\n",
       "                                                    'comment_text'),\n",
       "                                                   ('txt_char',\n",
       "                                                    TfidfVectorizer(analyzer='char',\n",
       "                                                                    dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(3,\n",
       "                                                                                 6)),\n",
       "                                                    'comment_text')])),\n",
       "                  ('clf',\n",
       "                   LogisticRegression(C=2.154434690031882, penalty='l1',\n",
       "                                      solver='saga'))])},\n",
       " 'threat': {'score': 0.9973757607744117,\n",
       "  'parameters': {'clf__C': 10.0, 'clf__penalty': 'l2', 'clf__solver': 'sag'},\n",
       "  'estimator': Pipeline(memory='tmp/cache',\n",
       "           steps=[('trans',\n",
       "                   ColumnTransformer(transformers=[('txt_word',\n",
       "                                                    TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 3)),\n",
       "                                                    'comment_text'),\n",
       "                                                   ('txt_char',\n",
       "                                                    TfidfVectorizer(analyzer='char',\n",
       "                                                                    dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(3,\n",
       "                                                                                 6)),\n",
       "                                                    'comment_text')])),\n",
       "                  ('clf', LogisticRegression(C=10.0, solver='sag'))])},\n",
       " 'insult': {'score': 0.9721360560814111,\n",
       "  'parameters': {'clf__C': 2.154434690031882,\n",
       "   'clf__penalty': 'l2',\n",
       "   'clf__solver': 'saga'},\n",
       "  'estimator': Pipeline(memory='tmp/cache',\n",
       "           steps=[('trans',\n",
       "                   ColumnTransformer(transformers=[('txt_word',\n",
       "                                                    TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 3)),\n",
       "                                                    'comment_text'),\n",
       "                                                   ('txt_char',\n",
       "                                                    TfidfVectorizer(analyzer='char',\n",
       "                                                                    dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(3,\n",
       "                                                                                 6)),\n",
       "                                                    'comment_text')])),\n",
       "                  ('clf',\n",
       "                   LogisticRegression(C=2.154434690031882, solver='saga'))])},\n",
       " 'identity_hate': {'score': 0.9926129590300082,\n",
       "  'parameters': {'clf__C': 2.154434690031882,\n",
       "   'clf__penalty': 'l2',\n",
       "   'clf__solver': 'sag'},\n",
       "  'estimator': Pipeline(memory='tmp/cache',\n",
       "           steps=[('trans',\n",
       "                   ColumnTransformer(transformers=[('txt_word',\n",
       "                                                    TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(1,\n",
       "                                                                                 3)),\n",
       "                                                    'comment_text'),\n",
       "                                                   ('txt_char',\n",
       "                                                    TfidfVectorizer(analyzer='char',\n",
       "                                                                    dtype=<class 'numpy.float32'>,\n",
       "                                                                    max_features=10000,\n",
       "                                                                    ngram_range=(3,\n",
       "                                                                                 6)),\n",
       "                                                    'comment_text')])),\n",
       "                  ('clf', LogisticRegression(C=2.154434690031882, solver='sag'))])}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.960213</td>\n",
       "      <td>0.990756</td>\n",
       "      <td>0.980988</td>\n",
       "      <td>0.997376</td>\n",
       "      <td>0.972136</td>\n",
       "      <td>0.992613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parameters</th>\n",
       "      <td>{'clf__C': 2.154434690031882, 'clf__penalty': 'l1', 'clf__solver': 'saga'}</td>\n",
       "      <td>{'clf__C': 2.154434690031882, 'clf__penalty': 'l2', 'clf__solver': 'sag'}</td>\n",
       "      <td>{'clf__C': 2.154434690031882, 'clf__penalty': 'l1', 'clf__solver': 'saga'}</td>\n",
       "      <td>{'clf__C': 10.0, 'clf__penalty': 'l2', 'clf__solver': 'sag'}</td>\n",
       "      <td>{'clf__C': 2.154434690031882, 'clf__penalty': 'l2', 'clf__solver': 'saga'}</td>\n",
       "      <td>{'clf__C': 2.154434690031882, 'clf__penalty': 'l2', 'clf__solver': 'sag'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimator</th>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, penalty='l1', solver='saga'))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, solver='sag'))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, penalty='l1', solver='saga'))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=10.0, solver='sag'))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, solver='saga'))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, solver='sag'))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               toxic  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.960213   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {'clf__C': 2.154434690031882, 'clf__penalty': 'l1', 'clf__solver': 'saga'}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, penalty='l1', solver='saga'))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         severe_toxic  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.990756   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {'clf__C': 2.154434690031882, 'clf__penalty': 'l2', 'clf__solver': 'sag'}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, solver='sag'))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             obscene  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.980988   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                {'clf__C': 2.154434690031882, 'clf__penalty': 'l1', 'clf__solver': 'saga'}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, penalty='l1', solver='saga'))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  threat  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0.997376   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {'clf__C': 10.0, 'clf__penalty': 'l2', 'clf__solver': 'sag'}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=10.0, solver='sag'))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                insult  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.972136   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {'clf__C': 2.154434690031882, 'clf__penalty': 'l2', 'clf__solver': 'saga'}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, solver='saga'))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        identity_hate  \n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.992613  \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {'clf__C': 2.154434690031882, 'clf__penalty': 'l2', 'clf__solver': 'sag'}  \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char',\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), LogisticRegression(C=2.154434690031882, solver='sag'))  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = pd.DataFrame.from_dict(best_results)\n",
    "final_results.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823470879122764"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.iloc[0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=2.154434690031882, penalty='l1',\n",
      "                                    solver='saga'))])\n",
      "severe_toxic Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', LogisticRegression(C=2.154434690031882, solver='sag'))])\n",
      "obscene Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=2.154434690031882, penalty='l1',\n",
      "                                    solver='saga'))])\n",
      "threat Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', LogisticRegression(C=10.0, solver='sag'))])\n",
      "insult Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf',\n",
      "                 LogisticRegression(C=2.154434690031882, solver='saga'))])\n",
      "identity_hate Pipeline(memory='tmp/cache',\n",
      "         steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', LogisticRegression(C=2.154434690031882, solver='sag'))])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "for k, v in best_results.items():\n",
    "    print(k, v['estimator'])\n",
    "    joblib.dump(v['estimator'], f'F:/Thesis/models/logistic/{k}.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "models = {}\n",
    "\n",
    "for label in labels:\n",
    "    models[label] = joblib.load(open(f'F:/Thesis/models/logistic/{label}.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.22s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.91s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.17s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------toxic-----------\n",
      "[[114140   1303]\n",
      " [  3760   8453]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    115443\n",
      "           1       0.87      0.69      0.77     12213\n",
      "\n",
      "    accuracy                           0.96    127656\n",
      "   macro avg       0.92      0.84      0.87    127656\n",
      "weighted avg       0.96      0.96      0.96    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.24s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 26.71s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.56s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 24.07s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.02s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------severe_toxic-----------\n",
      "[[126078    319]\n",
      " [   914    345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    126397\n",
      "           1       0.52      0.27      0.36      1259\n",
      "\n",
      "    accuracy                           0.99    127656\n",
      "   macro avg       0.76      0.64      0.68    127656\n",
      "weighted avg       0.99      0.99      0.99    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.86s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 25.31s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.13s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 24.00s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.02s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------obscene-----------\n",
      "[[120206    708]\n",
      " [  1790   4952]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    120914\n",
      "           1       0.87      0.73      0.80      6742\n",
      "\n",
      "    accuracy                           0.98    127656\n",
      "   macro avg       0.93      0.86      0.89    127656\n",
      "weighted avg       0.98      0.98      0.98    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.56s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 25.59s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.53s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.90s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------threat-----------\n",
      "[[127215     61]\n",
      " [   278    102]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    127276\n",
      "           1       0.63      0.27      0.38       380\n",
      "\n",
      "    accuracy                           1.00    127656\n",
      "   macro avg       0.81      0.63      0.69    127656\n",
      "weighted avg       1.00      1.00      1.00    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.84s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 24.18s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.67s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.62s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------insult-----------\n",
      "[[120453    907]\n",
      " [  2650   3646]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99    121360\n",
      "           1       0.80      0.58      0.67      6296\n",
      "\n",
      "    accuracy                           0.97    127656\n",
      "   macro avg       0.89      0.79      0.83    127656\n",
      "weighted avg       0.97      0.97      0.97    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.73s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.77s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.85s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.34s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.10s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------identity_hate-----------\n",
      "[[126358    168]\n",
      " [   801    329]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    126526\n",
      "           1       0.66      0.29      0.40      1130\n",
      "\n",
      "    accuracy                           0.99    127656\n",
      "   macro avg       0.83      0.64      0.70    127656\n",
      "weighted avg       0.99      0.99      0.99    127656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "for k, v in models.items():\n",
    "    y_actual = y_train[k]\n",
    "    y_predicted = cross_val_predict(v, X_train, y_train[k], cv=5)\n",
    "    print(f\"-----------{k}-----------\")\n",
    "    print(metrics.confusion_matrix(y_actual, y_predicted))\n",
    "    print(metrics.classification_report(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
