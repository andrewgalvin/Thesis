{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats of training set:  (159571, 7)\n",
      "Labels: ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Below option allows us to see the entire comment_text column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Read in the dataset\n",
    "train = pd.read_csv(\"../../data/kaggle_train.csv\")\n",
    "train = train.drop(columns=['id'])\n",
    "\n",
    "labels = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "print(\"Stats of training set: \", train.shape)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nMore\\r\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 Explanation\\r\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nMore\\r\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Below I have noticed some inconsistencies in the data and by preprocessing it, we can ensure a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert comment to lowercase\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(to_lowercase)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Remove HTML tags from the comments\n",
    "def remove_html(text):\n",
    "    return re.sub(r\"<.*>\", \"\", text, flags=re.MULTILINE)\n",
    "    \n",
    "train['comment_text'] = train['comment_text'].apply(remove_html)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                 explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted? they weren't vandalisms, just closure on some gas after i voted at new york dolls fac. and please don't remove the template from the talk page since i'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            d'aww! he matches this background colour i'm seemingly stuck with. thanks.  (talk) 21:51, january 11, 2016 (utc)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                   hey man, i'm really not trying to edit war. it's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. he seems to care more about the formatting than the actual info.   \n",
       "3  \"\\r\\nmore\\r\\ni can't make any real suggestions on improvement - i wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -i think the references may need tidying so that they are all in the exact same format ie date format etc. i can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up. it's listed in the relevant form eg wikipedia:good_article_nominations#transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you, sir, are my hero. any chance you remember what page that's on?   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove links from the comments\n",
    "def remove_links(text):\n",
    "    text= re.sub(r\"http\\S+\",\" \",text, flags=re.MULTILINE)\n",
    "    return re.sub(r\"www\\S+\",\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_links)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\r\\nmore\\r\\ni cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember what page thats on</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                      explanation\\r\\nwhy the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                    hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info   \n",
       "3  \\r\\nmore\\r\\ni cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know\\r\\n\\r\\nthere appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport     \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         you sir are my hero any chance you remember what page thats on   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "# Remove punctuation marks \n",
    "def remove_punctuation(text):\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i, \"\")\n",
    "    return text\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_punctuation)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation  why the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more  i cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know    there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember what page thats on</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    comment_text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                explanation  why the edits made under my username hardcore metallica fan were reverted they werent vandalisms just closure on some gas after i voted at new york dolls fac and please dont remove the template from the talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           daww he matches this background colour im seemingly stuck with thanks  talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                            hey man im really not trying to edit war its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info   \n",
       "3    more  i cant make any real suggestions on improvement  i wondered if the section statistics should be later on or a subsection of types of accidents  i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if noone else does first  if you have any preferences for formatting style on references or want to do it yourself please let me know    there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up its listed in the relevant form eg wikipediagoodarticlenominationstransport     \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 you sir are my hero any chance you remember what page thats on   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove special characters such as: \\n \\r \\t\n",
    "def remove_special(text):\n",
    "    return re.sub(r\"[\\n\\t\\\\\\/\\r]\",\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_special)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now892053827</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww matches background colour im seemingly stuck thanks talk 2151 january 11 2016 utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                comment_text  \\\n",
       "0                                                                                                                                                                                                 explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now892053827   \n",
       "1                                                                                                                                                                                                                                                                                                     daww matches background colour im seemingly stuck thanks talk 2151 january 11 2016 utc   \n",
       "2                                                                                                                                                                                                                                           hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info   \n",
       "3  cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport   \n",
       "4                                                                                                                                                                                                                                                                                                                                                        sir hero chance remember page thats   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords using nltk's stopwords package\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daww matches background colour im seemingly stuck thanks talk      january         utc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page thats</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                comment_text  \\\n",
       "0                                                                                                                                                                                                 explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired now            \n",
       "1                                                                                                                                                                                                                                                                                                     daww matches background colour im seemingly stuck thanks talk      january         utc   \n",
       "2                                                                                                                                                                                                                                           hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info   \n",
       "3  cant make real suggestions improvement wondered section statistics later subsection types accidents think references may need tidying exact format ie date format etc later noone else first preferences formatting style references want please let know appears backlog articles review guess may delay reviewer turns listed relevant form eg wikipediagoodarticlenominationstransport   \n",
       "4                                                                                                                                                                                                                                                                                                                                                        sir hero chance remember page thats   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0      0             0        0       0       0              0  \n",
       "1      0             0        0       0       0              0  \n",
       "2      0             0        0       0       0              0  \n",
       "3      0             0        0       0       0              0  \n",
       "4      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As you can see above, there are numbers and/or dates\n",
    "# I will remove those as they are not helpful\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d',\" \",text, flags=re.MULTILINE)\n",
    "\n",
    "train['comment_text'] = train['comment_text'].apply(remove_numbers)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Test, and Validation Split\n",
    "\n",
    "Below I need to split the dataset into train and test datasets.\n",
    "However, sklearn's `train_test_split` function does not work for\n",
    "multi-class classification.\n",
    "\n",
    "Therefore, I will be creating a train, test, and validation split for each label in the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64330</th>\n",
       "      <td>saved lives well delayed death serious injury shell finally hit trench ordered top attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99454</th>\n",
       "      <td>estimate total biomass incorrect page defines biomass biomass organic nonfossil material collectively words biomass describes mass biological organisms dead alive goes state entire earth contains    billion tons biomass assuming   billion     estimate low consistent definition give example global forest resources assessment      fao estimates global total aboveground woody biomass     billion tonnes david wardle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44699</th>\n",
       "      <td>thanks fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9616</th>\n",
       "      <td>song adaptation worthes theres song iron maiden movie heart darkness called edge darkness      x factor album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98170</th>\n",
       "      <td>thats interesting however fact pira murdered around      maimed around       people single important fact organistion must included within lead pira article real issue ensuring correct verifiable sources figures dead injured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89688</th>\n",
       "      <td>still stupid cunt whore still stupid cunt whore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46589</th>\n",
       "      <td>happens may worth noting present view like advert encyclopedic entry macdui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101304</th>\n",
       "      <td>apologize giving benefit doubt someone drew borderline pedophilic image</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36347</th>\n",
       "      <td>im sorry must misread article look back absense mention wmd bush telling public iraq suddenly npov pov silly link iraq     terrorists oh wait right included negative things bush forgot cant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130733</th>\n",
       "      <td>valentines day best friend anyways happy mandatory love day chronic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127656 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                           comment_text\n",
       "64330                                                                                                                                                                                                                                                                                                                                         saved lives well delayed death serious injury shell finally hit trench ordered top attack\n",
       "99454   estimate total biomass incorrect page defines biomass biomass organic nonfossil material collectively words biomass describes mass biological organisms dead alive goes state entire earth contains    billion tons biomass assuming   billion     estimate low consistent definition give example global forest resources assessment      fao estimates global total aboveground woody biomass     billion tonnes david wardle\n",
       "44699                                                                                                                                                                                                                                                                                                                                                                                                                       thanks fuck\n",
       "9616                                                                                                                                                                                                                                                                                                                      song adaptation worthes theres song iron maiden movie heart darkness called edge darkness      x factor album\n",
       "98170                                                                                                                                                                                                  thats interesting however fact pira murdered around      maimed around       people single important fact organistion must included within lead pira article real issue ensuring correct verifiable sources figures dead injured\n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                 ...\n",
       "89688                                                                                                                                                                                                                                                                                                                                                                                   still stupid cunt whore still stupid cunt whore\n",
       "46589                                                                                                                                                                                                                                                                                                                                                       happens may worth noting present view like advert encyclopedic entry macdui\n",
       "101304                                                                                                                                                                                                                                                                                                                                                          apologize giving benefit doubt someone drew borderline pedophilic image\n",
       "36347                                                                                                                                                                                                                                     im sorry must misread article look back absense mention wmd bush telling public iraq suddenly npov pov silly link iraq     terrorists oh wait right included negative things bush forgot cant\n",
       "130733                                                                                                                                                                                                                                                                                                                                                              valentines day best friend anyways happy mandatory love day chronic\n",
       "\n",
       "[127656 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[[\"comment_text\"]], train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]], test_size=0.20)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (127656, 1)\n",
      "Test shape: (31915, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\",X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing the Comment Text\n",
    "\n",
    "*Logistic Regression can't take text values as input*\n",
    "\n",
    "Since the independent variable I have is only text, we will need to use a vectorizer to convert the text into usable data for Logistic Regression.\n",
    "\n",
    "```\n",
    "\n",
    "# Max_features = Build a vocabulary that only consider the top max_features ordered by term frequency\n",
    "\n",
    "# Analyzer = Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "\n",
    "# ngram_range = (1,1) means only unigrams, (1,2) means unigrams and bigrams, (1,3) means unigrams, bigrams, and trigrams\n",
    "\n",
    "# Further ngrams knowledge = bigrams means it will learn the occurence of every two words, trigrams would be every 3, etc.\n",
    "\n",
    "# dtype = type of the matrix returned, default is float64\n",
    "```\n",
    "\n",
    "We will use a word and char n-grams as some people like to obfuscate words by using multiple characters, by using both we can hope to catch these.\n",
    "The idea from this came from [here](https://www.kaggle.com/code/tunguz/logistic-regression-with-words-and-char-n-grams/comments) which has one of the best results for Logistic Regression. This user optimized the ngram_range.\n",
    "\n",
    "We use FeatureUnion (similar to how hstack works in previous non-Pipeline example) to combine the word and char n-ngrams as described in this [post](https://stackoverflow.com/questions/65765954/word-and-char-ngram-with-different-ngram-range-on-tfidfvectorizer-pipeline) into one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "cols_trans = ColumnTransformer([\n",
    "    (\"txt_word\", TfidfVectorizer(max_features=10000, binary=True, analyzer=\"word\", ngram_range=(1,3), dtype=np.float32), 'comment_text'),\n",
    "    (\"txt_char\", TfidfVectorizer(max_features=10000, binary=True, analyzer=\"char\", ngram_range=(3,6), dtype=np.float32), 'comment_text')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline \n",
    "\n",
    "Create a Pipeline for the data to flow through:\n",
    "\n",
    "TFIDF Vectorize the data\n",
    "\n",
    "then\n",
    "\n",
    "Perform Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('trans', cols_trans),\n",
    "    ('clf', BernoulliNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;trans&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                                  TfidfVectorizer(binary=True,\n",
       "                                                                  dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  &#x27;comment_text&#x27;),\n",
       "                                                 (&#x27;txt_char&#x27;,\n",
       "                                                  TfidfVectorizer(analyzer=&#x27;char&#x27;,\n",
       "                                                                  binary=True,\n",
       "                                                                  dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  &#x27;comment_text&#x27;)])),\n",
       "                (&#x27;clf&#x27;, BernoulliNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;trans&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                                  TfidfVectorizer(binary=True,\n",
       "                                                                  dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  &#x27;comment_text&#x27;),\n",
       "                                                 (&#x27;txt_char&#x27;,\n",
       "                                                  TfidfVectorizer(analyzer=&#x27;char&#x27;,\n",
       "                                                                  binary=True,\n",
       "                                                                  dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  &#x27;comment_text&#x27;)])),\n",
       "                (&#x27;clf&#x27;, BernoulliNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">trans: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;txt_word&#x27;,\n",
       "                                 TfidfVectorizer(binary=True,\n",
       "                                                 dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                 max_features=10000,\n",
       "                                                 ngram_range=(1, 3)),\n",
       "                                 &#x27;comment_text&#x27;),\n",
       "                                (&#x27;txt_char&#x27;,\n",
       "                                 TfidfVectorizer(analyzer=&#x27;char&#x27;, binary=True,\n",
       "                                                 dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                                                 max_features=10000,\n",
       "                                                 ngram_range=(3, 6)),\n",
       "                                 &#x27;comment_text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">txt_word</label><div class=\"sk-toggleable__content\"><pre>comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(binary=True, dtype=&lt;class &#x27;numpy.float32&#x27;&gt;, max_features=10000,\n",
       "                ngram_range=(1, 3))</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">txt_char</label><div class=\"sk-toggleable__content\"><pre>comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(analyzer=&#x27;char&#x27;, binary=True, dtype=&lt;class &#x27;numpy.float32&#x27;&gt;,\n",
       "                max_features=10000, ngram_range=(3, 6))</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('trans',\n",
       "                 ColumnTransformer(transformers=[('txt_word',\n",
       "                                                  TfidfVectorizer(binary=True,\n",
       "                                                                  dtype=<class 'numpy.float32'>,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(1,\n",
       "                                                                               3)),\n",
       "                                                  'comment_text'),\n",
       "                                                 ('txt_char',\n",
       "                                                  TfidfVectorizer(analyzer='char',\n",
       "                                                                  binary=True,\n",
       "                                                                  dtype=<class 'numpy.float32'>,\n",
       "                                                                  max_features=10000,\n",
       "                                                                  ngram_range=(3,\n",
       "                                                                               6)),\n",
       "                                                  'comment_text')])),\n",
       "                ('clf', BernoulliNB())])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "# with display='diagram', simply use display() to see the diagram\n",
    "display(pipe)\n",
    "# if desired, set display back to the default\n",
    "set_config(display='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "\n",
    "Below I will build and train the `Logistic Regression` model and check if the model is overfit, underfit, or optimal fit using GridSearch I will find the best hyperparameters.\n",
    "\n",
    "We create a new model for each label in order to classify the multi-class, example we cross validate the `toxic` label, then the `severe_toxic` and so on. This method is the preferred method based on previous implementations for the Kaggle competition.\n",
    "\n",
    "By  doing this, we can evaluate the percentage for each label and choose the highest label(s) which we should classify the text as. For example, in the data we have data which may be `toxic` and `obscene` rather than only `toxic` data and only `obscene` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha Values: \n",
      "0.0\n",
      "0.001\n",
      "0.004641588833612777\n",
      "0.021544346900318832\n",
      "0.1\n",
      "0.46415888336127775\n",
      "2.154434690031882\n",
      "10.0\n",
      "46.41588833612773\n",
      "215.44346900318823\n",
      "1000.0\n"
     ]
    }
   ],
   "source": [
    "# speecify parameter values to search\n",
    "alpha = np.logspace(-3, 3, 10)\n",
    "alpha = np.append(0, alpha)\n",
    "\n",
    "print(\"Alpha Values: \")\n",
    "for c in alpha:\n",
    "    print(c)\n",
    "params = {}\n",
    "params['clf__alpha'] = alpha\n",
    "params['clf__fit_prior'] = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tune [toxic]: 9947.086345672607\n",
      "\tBest Score: 0.9019082523638465\n",
      "\tFinal Model:: Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=215.44346900318823))])\n",
      "Time to tune [severe_toxic]: 9517.49910068512\n",
      "\tBest Score: 0.988766685525799\n",
      "\tFinal Model:: Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n",
      "Time to tune [obscene]: 9425.910165071487\n",
      "\tBest Score: 0.9320126022268683\n",
      "\tFinal Model:: Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n",
      "Time to tune [threat]: 9642.61752820015\n",
      "\tBest Score: 0.99703108359764\n",
      "\tFinal Model:: Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n",
      "Time to tune [insult]: 9571.185094833374\n",
      "\tBest Score: 0.9358118786490837\n",
      "\tFinal Model:: Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n",
      "Time to tune [identity_hate]: 9549.2076253891\n",
      "\tBest Score: 0.990544902105689\n",
      "\tFinal Model:: Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "best_results = {}\n",
    "\n",
    "for label in labels:\n",
    "    start = time.time()\n",
    "\n",
    "    grid = GridSearchCV(pipe, params, cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train[label])\n",
    "\n",
    "    best_results[label] = {\n",
    "        \"score\": grid.best_score_,\n",
    "        \"parameters\": grid.best_params_,\n",
    "        \"estimator\": grid.best_estimator_\n",
    "    }\n",
    "\n",
    "    print(f\"Time to tune [{label}]: {time.time() - start}\")\n",
    "    print(f\"\\tBest Score: {grid.best_score_}\")\n",
    "    print(f\"\\tFinal Model:: {grid.best_estimator_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.901908</td>\n",
       "      <td>0.988767</td>\n",
       "      <td>0.932013</td>\n",
       "      <td>0.997031</td>\n",
       "      <td>0.935812</td>\n",
       "      <td>0.990545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parameters</th>\n",
       "      <td>{'clf__alpha': 215.44346900318823, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 1000.0, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 1000.0, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 1000.0, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 1000.0, 'clf__fit_prior': True}</td>\n",
       "      <td>{'clf__alpha': 1000.0, 'clf__fit_prior': True}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estimator</th>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=215.44346900318823))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))</td>\n",
       "      <td>(ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=&lt;class 'numpy.float32'&gt;,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            toxic  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    0.901908   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'clf__alpha': 215.44346900318823, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=215.44346900318823))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         severe_toxic  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.988767   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'clf__alpha': 1000.0, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              obscene  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.932013   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'clf__alpha': 1000.0, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               threat  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.997031   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'clf__alpha': 1000.0, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               insult  \\\n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.935812   \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'clf__alpha': 1000.0, 'clf__fit_prior': True}   \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        identity_hate  \n",
       "score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.990545  \n",
       "parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             {'clf__alpha': 1000.0, 'clf__fit_prior': True}  \n",
       "estimator   (ColumnTransformer(transformers=[('txt_word',\\n                                 TfidfVectorizer(binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(1, 3)),\\n                                 'comment_text'),\\n                                ('txt_char',\\n                                 TfidfVectorizer(analyzer='char', binary=True,\\n                                                 dtype=<class 'numpy.float32'>,\\n                                                 max_features=10000,\\n                                                 ngram_range=(3, 6)),\\n                                 'comment_text')]), BernoulliNB(alpha=1000.0))  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = pd.DataFrame.from_dict(best_results)\n",
    "final_results.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9576792340781544"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.iloc[0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=215.44346900318823))])\n",
      "severe_toxic Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n",
      "obscene Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n",
      "threat Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n",
      "insult Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n",
      "identity_hate Pipeline(steps=[('trans',\n",
      "                 ColumnTransformer(transformers=[('txt_word',\n",
      "                                                  TfidfVectorizer(binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(1,\n",
      "                                                                               3)),\n",
      "                                                  'comment_text'),\n",
      "                                                 ('txt_char',\n",
      "                                                  TfidfVectorizer(analyzer='char',\n",
      "                                                                  binary=True,\n",
      "                                                                  dtype=<class 'numpy.float32'>,\n",
      "                                                                  max_features=10000,\n",
      "                                                                  ngram_range=(3,\n",
      "                                                                               6)),\n",
      "                                                  'comment_text')])),\n",
      "                ('clf', BernoulliNB(alpha=1000.0))])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "for k, v in best_results.items():\n",
    "    print(k, v['estimator'])\n",
    "    joblib.dump(v['estimator'], f'F:/Thesis/models/bernoulli_naive/{k}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "models = {}\n",
    "\n",
    "for label in labels:\n",
    "    models[label] = joblib.load(open(f'F:/Thesis/models/logistic/{label}.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.29s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.49s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.34s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.31s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.62s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------toxic-----------\n",
      "[[114070   1307]\n",
      " [  3768   8511]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    115377\n",
      "           1       0.87      0.69      0.77     12279\n",
      "\n",
      "    accuracy                           0.96    127656\n",
      "   macro avg       0.92      0.84      0.87    127656\n",
      "weighted avg       0.96      0.96      0.96    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 20.78s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.76s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.08s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.25s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------severe_toxic-----------\n",
      "[[126063    319]\n",
      " [   899    375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    126382\n",
      "           1       0.54      0.29      0.38      1274\n",
      "\n",
      "    accuracy                           0.99    127656\n",
      "   macro avg       0.77      0.65      0.69    127656\n",
      "weighted avg       0.99      0.99      0.99    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.47s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.30s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.94s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 23.08s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.43s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------obscene-----------\n",
      "[[120201    658]\n",
      " [  1756   5041]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    120859\n",
      "           1       0.88      0.74      0.81      6797\n",
      "\n",
      "    accuracy                           0.98    127656\n",
      "   macro avg       0.94      0.87      0.90    127656\n",
      "weighted avg       0.98      0.98      0.98    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.44s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.62s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.07s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.97s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.84s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------threat-----------\n",
      "[[127227     61]\n",
      " [   275     93]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    127288\n",
      "           1       0.60      0.25      0.36       368\n",
      "\n",
      "    accuracy                           1.00    127656\n",
      "   macro avg       0.80      0.63      0.68    127656\n",
      "weighted avg       1.00      1.00      1.00    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.69s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 22.49s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.53s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.55s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 19.21s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------insult-----------\n",
      "[[120391    919]\n",
      " [  2649   3697]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99    121310\n",
      "           1       0.80      0.58      0.67      6346\n",
      "\n",
      "    accuracy                           0.97    127656\n",
      "   macro avg       0.89      0.79      0.83    127656\n",
      "weighted avg       0.97      0.97      0.97    127656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.62s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.61s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.58s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n",
      "c:\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:336: UserWarning: Persisting input arguments took 21.41s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  X, fitted_transformer = fit_transform_one_cached(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------identity_hate-----------\n",
      "[[126368    154]\n",
      " [   812    322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00    126522\n",
      "           1       0.68      0.28      0.40      1134\n",
      "\n",
      "    accuracy                           0.99    127656\n",
      "   macro avg       0.84      0.64      0.70    127656\n",
      "weighted avg       0.99      0.99      0.99    127656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "for k, v in models.items():\n",
    "    y_actual = y_train[k]\n",
    "    y_predicted = cross_val_predict(v, X_train, y_train[k], cv=5)\n",
    "    print(f\"-----------{k}-----------\")\n",
    "    print(metrics.confusion_matrix(y_actual, y_predicted))\n",
    "    print(metrics.classification_report(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
